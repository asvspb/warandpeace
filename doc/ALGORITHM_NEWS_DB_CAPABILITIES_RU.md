### Возможности после реализации плана ALGORITHM_NEWS_DB

Ниже собраны новые возможности проекта после выделения инжеста «сырых» данных и суммаризации, внедрения DLQ, метрик и инструментов контроля качества данных.

#### 1) Разделение конвейеров: инжест отдельно от суммаризации
- Инжест теперь сохраняет «сырые» статьи: `title`, `canonical_link`, `published_at`, полный очищенный `content`.
- Суммаризация запускается отдельными командами и не блокирует сбор новостей.
- Идемпотентность: upsert по `canonical_link`, повторные записи не дублируются.

#### 2) Новые CLI-команды
- Инжест актуальной страницы ленты:
```bash
python scripts/manage.py ingest-page --page 1 --limit 10
```
- Исторический backfill диапазона дат (без суммаризации):
```bash
python scripts/manage.py backfill-range --from-date 2025-08-01 --to-date 2025-08-07
```
- Ночной reconcile (сверка и дозагрузка за последние N дней):
```bash
python scripts/manage.py reconcile --since-days 7
```
- Суммаризация статей без резюме за диапазон:
```bash
python scripts/manage.py summarize-range --from-date 2025-08-01 --to-date 2025-08-07
```
- Управление очередью проблемных элементов (DLQ):
```bash
# Просмотр содержимого DLQ
python scripts/manage.py dlq-show --type all --limit 50

# Повторная обработка статей из DLQ (пакетно)
python scripts/manage.py dlq-retry --type article --limit 20
```
- Отчёты по дубликатам контента:
```bash
# Группы с одинаковым content_hash
python scripts/manage.py content-hash-report --min-count 2 --details

# Поиск почти-дубликатов (шинглы + Jaccard)
python scripts/manage.py near-duplicates --days 7 --limit 200 --threshold 0.8
```

#### 3) Качество данных и дедупликация
- Каноникализация URL при любом сохранении (см. `src/url_utils.py`).
- Поле `content_hash` (SHA-256) используется для точной дедупликации и отчётов.
- Команда `near-duplicates` помогает выявлять похожие статьи для ручного аудита.

#### 4) DLQ (Dead-letter Queue)
- Все постоянные ошибки инжеста попадают в `dlq` с полями `entity_type`, `entity_ref`, `error_code`, `attempts`.
- Доступны команды `dlq-show` и `dlq-retry` (пока для `article`).
- Размер очереди отражается метрикой `dlq_size`. Команды CLI печатают итог с `DLQ=<число>`.

#### 5) Метрики и наблюдаемость (Prometheus)
- Включение через переменные окружения: `METRICS_ENABLED=true`, `METRICS_PORT=8000` (по умолчанию).
- Эндпоинт: `http://<host>:8000/metrics` (в Docker порт проброшен).
- Экспортируются метрики:
  - `articles_ingested_total` — количество инжестированных «сырых» статей
  - `articles_posted_total` — количество опубликованных в Telegram статей
  - `errors_total{type}` — количество ошибок по типам
  - `job_duration_seconds` — длительность фоновой задачи
  - `last_article_age_minutes` — «возраст» последней статьи в БД
  - `dlq_size` — размер очереди DLQ
- В тестовой среде при отсутствии `prometheus_client` метрики деградируют «без шума» (no-op).

#### 6) Миграции и надёжность
- Обновление схемы обёрнуто в транзакции; перед сложными миграциями создаётся файловый бэкап БД (`backup_database_copy`).
- Добавлены индексы (`published_at`, `backfill_status`, `content_hash`).
- Команды инжеста используют ретраи с экспоненциальной задержкой (`tenacity`).

#### 7) Рецепты запуска
- Быстрый сбор последних новостей + суммаризация:
```bash
python scripts/manage.py ingest-page --page 1 --limit 10
python scripts/manage.py summarize-range --from-date 2025-08-14 --to-date 2025-08-14
```
- Ночной режим (cron/джоб):
```bash
python scripts/manage.py reconcile --since-days 1
python scripts/manage.py summarize-range --from-date 2025-08-13 --to-date 2025-08-14
```
- Диагностика:
```bash
python scripts/manage.py dlq-show --type all --limit 100
python scripts/manage.py content-hash-report --min-count 2 --details
python scripts/manage.py near-duplicates --days 14 --limit 300 --threshold 0.75
```

#### 8) Ограничения и планы
- `dlq-retry` для `summary` пока не реализован.
- Параллелизм инжеста в CLI ограничен; в планах bounded parallelism.
- Для почти-дубликатов используется простой шингловый подход; возможен переход на MinHash/LSH.

## Простыми словами: что всё это значит?

Если отбросить технические детали, система эволюционировала в мощный новостной комбайн с повышенной надёжностью и удобным контролем.

**Ключевые улучшения:**

1.  **Скорость и надёжность:** Сбор новостей и создание кратких пересказов теперь — два независимых процесса. Бот может быстро собирать все статьи с сайта, не ожидая, пока ИИ напишет для них резюме. Это делает систему значительно быстрее и устойчивее к сбоям.

2.  **Путешествия во времени:** Появились мощные инструменты для управления архивом. Можно не только загружать свежие новости, но и забирать статьи за любую прошлую неделю или месяц (`backfill-range`). А специальная команда (`reconcile`) сама проверит, не упустил ли бот что-то за последние дни.

3.  **Умный поиск дубликатов:** Система научилась находить не только статьи с одинаковыми ссылками, но и почти идентичные тексты, даже если их заголовки или адреса немного отличаются. Это обеспечивает высокое качество контента и избавляет от повторов.

4.  **"Карантин" для проблемных новостей:** Если статью не удалось обработать (например, из-за ошибки на сайте), она больше не теряется. Вместо этого она попадает в специальную "очередь для проблемных" (`DLQ`). Оттуда её можно проанализировать и обработать повторно, не останавливая работу всего механизма.

5.  **"Приборная панель" для контроля:** У системы появились метрики (Prometheus), которые в реальном времени показывают её "здоровье": сколько статей обработано, сколько возникло ошибок, как быстро всё работает. Это как спидометр и датчики в автомобиле — позволяет держать всё под контролем.

6.  **Защита от потерь данных:** Перед любыми важными изменениями в своей структуре база данных теперь автоматически создаёт резервную копию. Ваши данные в безопасности.

В итоге, вы получаете не просто бота, а профессиональный инструмент для создания новостного канала, который работает быстро, эффективно и даёт вам полный контроль над процессом.
